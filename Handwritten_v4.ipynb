{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Handwritten_v4.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ashishjaiswal181/Google-Colab-Programs/blob/master/Handwritten_v4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "k75FMmNcbFgz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "ed193007-772e-4661-97c5-b3c19740fd7c"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5nFFp2hJbPeP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import h5py\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.framework import ops\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "np.random.seed(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uVK5COsGeYdc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x=pd.read_csv('/content/gdrive/My Drive/Font/A_Z_Handwritten_Data.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yAK-NJ6AeuwU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f5427be9-273c-4558-ddb4-7614ac4f6f32"
      },
      "cell_type": "code",
      "source": [
        "x.shape #Checking the shape of the dataset"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(372450, 785)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "JZtvMCrofB7t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data=x.iloc[:,1:372449] #Extracting the image data seperately from the data that contains image data and label data together\n",
        "data = np.array(data[1:372449], dtype=np.int) #Converting the data into numpy array\n",
        "data = data.T\n",
        "#data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ixYYhQFVfDRb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d40bb71a-7412-41c1-d0f4-e56c20284b7c"
      },
      "cell_type": "code",
      "source": [
        "data.shape #Verifying the shape of the data"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(784, 372448)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "TMSWQ076fGTu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y=x['0'] #Extracting the labels from the dataset\n",
        "y = np.array(y[1:372449], dtype=np.int)[np.newaxis] #Converting the labels into numpy array; 1D array to 2D array\n",
        "#y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_fSy8HJqfJ8X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "83b56de5-f678-4b79-dab7-6a1c63f4587a"
      },
      "cell_type": "code",
      "source": [
        "y.shape #Verifying the shape of the labels"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 372448)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "ntHOnL49faya",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "675d605d-c530-4987-935d-6484ec7dea0f"
      },
      "cell_type": "code",
      "source": [
        "classes=['A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z'] #defining the character classes\n",
        "index=9999 #To the check the corresponding character at respective index\n",
        "print (\"y = \"+str(y[0,index])+\". It's a '\"+classes[y[0,index]]+\"' character.\") #Printing the value of Y(label) and the corresponding character from the class"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y = 0. It's a 'A' character.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "e1q1rn3AivwN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "2b04a641-84e2-4659-9deb-03c3334f6972"
      },
      "cell_type": "code",
      "source": [
        "# Explore your dataset \n",
        "# Train_x = data\n",
        "# Tarin_y = y\n",
        "m_train = data.shape[1]\n",
        "num_px = 28\n",
        "#Dimension of x =28*28 = 784 = nx\n",
        "print (\"Number of training examples: \" + str(m_train))\n",
        "print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
        "print (\"train_x shape: \" + str(data.shape))\n",
        "print (\"train_y shape: \" + str(y.shape))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 372448\n",
            "Each image is of size: (28, 28, 3)\n",
            "train_x shape: (784, 372448)\n",
            "train_y shape: (1, 372448)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LEd9SzysfpLs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def convert_to_one_hot(Y, C):\n",
        "    Y = np.eye(C)[Y.reshape(-1)].T\n",
        "    return Y\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "56IXzi2cjYGj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3048fcfe-e34c-4f36-aa72-08f2e9aea4c5"
      },
      "cell_type": "code",
      "source": [
        "# Reshape the training and test examples \n",
        "train_x_flatten = data.reshape(data.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\n",
        "#test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
        "\n",
        "# Standardize data to have feature values between 0 and 1.\n",
        "train_x = train_x_flatten/255.\n",
        "#test_x = test_x_flatten/255.\n",
        "# Convert training and test labels to one hot matrices\n",
        "y = convert_to_one_hot(y, 26)\n",
        "#Y_test = convert_to_one_hot(Y_test_orig, 26)\n",
        "\n",
        "print (\"train_x shape: \" + str(data.shape))\n",
        "print (\"train_y shape: \" + str(y.shape))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_x shape: (784, 372448)\n",
            "train_y shape: (26, 372448)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mDw_B_uYj74c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# GRADED FUNCTION: create_placeholders\n",
        "\n",
        "def create_placeholders(n_x, n_y):\n",
        "    \"\"\"\n",
        "    Creates the placeholders for the tensorflow session.\n",
        "    \n",
        "    Arguments:\n",
        "    n_x -- scalar, size of an image vector (num_px * num_px = 64 * 64 * 3 = 12288)\n",
        "    n_y -- scalar, number of classes (from 0 to 5, so -> 6)\n",
        "    \n",
        "    Returns:\n",
        "    X -- placeholder for the data input, of shape [n_x, None] and dtype \"float\"\n",
        "    Y -- placeholder for the input labels, of shape [n_y, None] and dtype \"float\"\n",
        "    \n",
        "    Tips:\n",
        "    - You will use None because it let's us be flexible on the number of examples you will for the placeholders.\n",
        "      In fact, the number of examples during test/train is different.\n",
        "    \"\"\"\n",
        "\n",
        "    ### START CODE HERE ### (approx. 2 lines)\n",
        "    X = tf.placeholder(tf.float32,shape=(n_x,None),name=\"X\")\n",
        "    Y = tf.placeholder(tf.float32,shape=(n_y,None),name=\"Y\")\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return X, Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uA_LCze4j9Q6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# GRADED FUNCTION: initialize_parameters\n",
        "\n",
        "def initialize_parameters():\n",
        "    \"\"\"\n",
        "    Initializes parameters with xavier initializer to build a neural network with tensorflow. The shapes are:\n",
        "                        W1 : [25, 12288]\n",
        "                        b1 : [25, 1]\n",
        "                        W2 : [12, 25]\n",
        "                        b2 : [12, 1]\n",
        "                        W3 : [6, 12]\n",
        "                        b3 : [6, 1]\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- a dictionary of tensors containing W1, b1, W2, b2, W3, b3\n",
        "    \"\"\"\n",
        "    \n",
        "    tf.set_random_seed(1)                   # so that your \"random\" numbers match ours\n",
        "        \n",
        "    ### START CODE HERE ### (approx. 6 lines of code)\n",
        "    W1 = tf.get_variable(\"W1\", [100,784], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
        "    b1 = tf.get_variable(\"b1\", [100,1], initializer = tf.zeros_initializer())\n",
        "    W2 = tf.get_variable(\"W2\", [50,100], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
        "    b2 = tf.get_variable(\"b2\", [50,1], initializer = tf.zeros_initializer())\n",
        "    W3 = tf.get_variable(\"W3\", [26,50], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
        "    b3 = tf.get_variable(\"b3\", [26,1], initializer = tf.zeros_initializer())\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    parameters = {\"W1\": W1,\n",
        "                  \"b1\": b1,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2,\n",
        "                  \"W3\": W3,\n",
        "                  \"b3\": b3}\n",
        "    \n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ruozteQUkxDn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# GRADED FUNCTION: forward_propagation\n",
        "\n",
        "def forward_propagation(X, parameters):\n",
        "    \"\"\"\n",
        "    Implements the forward propagation for the model: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX\n",
        "    \n",
        "    Arguments:\n",
        "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
        "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"\n",
        "                  the shapes are given in initialize_parameters\n",
        "\n",
        "    Returns:\n",
        "    Z3 -- the output of the last LINEAR unit\n",
        "    \"\"\"\n",
        "    \n",
        "    # Retrieve the parameters from the dictionary \"parameters\" \n",
        "    W1 = parameters['W1']\n",
        "    b1 = parameters['b1']\n",
        "    W2 = parameters['W2']\n",
        "    b2 = parameters['b2']\n",
        "    W3 = parameters['W3']\n",
        "    b3 = parameters['b3']\n",
        "    \n",
        "    ### START CODE HERE ### (approx. 5 lines)              # Numpy Equivalents:\n",
        "    Z1 = tf.add(tf.matmul(W1,X),b1)                                              # Z1 = np.dot(W1, X) + b1\n",
        "    A1 = tf.nn.relu(Z1)                                              # A1 = relu(Z1)\n",
        "    Z2 = tf.add(tf.matmul(W2,A1),b2)                                              # Z2 = np.dot(W2, a1) + b2\n",
        "    A2 = tf.nn.relu(Z2)                                              # A2 = relu(Z2)\n",
        "    Z3 = tf.add(tf.matmul(W3,A2),b3)                                              # Z3 = np.dot(W3,Z2) + b3\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return Z3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aEaqZxSMkzi0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# GRADED FUNCTION: compute_cost \n",
        "\n",
        "def compute_cost(Z3, Y):\n",
        "    \"\"\"\n",
        "    Computes the cost\n",
        "    \n",
        "    Arguments:\n",
        "    Z3 -- output of forward propagation (output of the last LINEAR unit), of shape (6, number of examples)\n",
        "    Y -- \"true\" labels vector placeholder, same shape as Z3\n",
        "    \n",
        "    Returns:\n",
        "    cost - Tensor of the cost function\n",
        "    \"\"\"\n",
        "    \n",
        "    # to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)\n",
        "    logits = tf.transpose(Z3)\n",
        "    labels = tf.transpose(Y)\n",
        "    \n",
        "    ### START CODE HERE ### (1 line of code)\n",
        "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = logits, labels = labels))\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return cost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UCY_n0ivjXyj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
        "    \"\"\"\n",
        "    Creates a list of random minibatches from (X, Y)\n",
        "    \n",
        "    Arguments:\n",
        "    X -- input data, of shape (input size, number of examples)\n",
        "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
        "    mini_batch_size - size of the mini-batches, integer\n",
        "    seed -- this is only for the purpose of grading, so that you're \"random minibatches are the same as ours.\n",
        "    \n",
        "    Returns:\n",
        "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
        "    \"\"\"\n",
        "    \n",
        "    m = X.shape[1]                  # number of training examples\n",
        "    mini_batches = []\n",
        "    np.random.seed(seed)\n",
        "    \n",
        "    # Step 1: Shuffle (X, Y)\n",
        "    permutation = list(np.random.permutation(m))\n",
        "    shuffled_X = X[:, permutation]\n",
        "    shuffled_Y = Y[:, permutation].reshape((Y.shape[0],m))\n",
        "\n",
        "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
        "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
        "    for k in range(0, num_complete_minibatches):\n",
        "        mini_batch_X = shuffled_X[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
        "        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
        "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
        "        mini_batches.append(mini_batch)\n",
        "    \n",
        "    # Handling the end case (last mini-batch < mini_batch_size)\n",
        "    if m % mini_batch_size != 0:\n",
        "        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size : m]\n",
        "        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size : m]\n",
        "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
        "        mini_batches.append(mini_batch)\n",
        "    \n",
        "    return mini_batches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r64Wus3MfsXN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def predict(X, parameters):\n",
        "    \n",
        "    W1 = tf.convert_to_tensor(parameters[\"W1\"])\n",
        "    b1 = tf.convert_to_tensor(parameters[\"b1\"])\n",
        "    W2 = tf.convert_to_tensor(parameters[\"W2\"])\n",
        "    b2 = tf.convert_to_tensor(parameters[\"b2\"])\n",
        "    W3 = tf.convert_to_tensor(parameters[\"W3\"])\n",
        "    b3 = tf.convert_to_tensor(parameters[\"b3\"])\n",
        "    \n",
        "    params = {\"W1\": W1,\n",
        "              \"b1\": b1,\n",
        "              \"W2\": W2,\n",
        "              \"b2\": b2,\n",
        "              \"W3\": W3,\n",
        "              \"b3\": b3}\n",
        "    \n",
        "    x = tf.placeholder(\"float\", [12288, 1])\n",
        "    \n",
        "    z3 = forward_propagation(x, params)\n",
        "    p = tf.argmax(z3)\n",
        "    \n",
        "    with tf.Session() as sess:\n",
        "        prediction = sess.run(p, feed_dict = {x: X})\n",
        "        \n",
        "    return prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9tXmO8w_f2s9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def model(X_train, Y_train, learning_rate = 0.0001,\n",
        "          num_epochs = 100, minibatch_size = 32, print_cost = True):\n",
        "    \"\"\"\n",
        "    def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.0001,\n",
        "          num_epochs = 1500, minibatch_size = 32, print_cost = True):\n",
        "    Implements a three-layer tensorflow neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SOFTMAX.\n",
        "    \n",
        "    Arguments:\n",
        "    X_train -- training set, of shape (input size = 12288, number of training examples = 1080)\n",
        "    Y_train -- test set, of shape (output size = 6, number of training examples = 1080)\n",
        "    X_test -- training set, of shape (input size = 12288, number of training examples = 120)\n",
        "    Y_test -- test set, of shape (output size = 6, number of test examples = 120)\n",
        "    learning_rate -- learning rate of the optimization\n",
        "    num_epochs -- number of epochs of the optimization loop\n",
        "    minibatch_size -- size of a minibatch\n",
        "    print_cost -- True to print the cost every 100 epochs\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
        "    \"\"\"\n",
        "    \n",
        "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
        "    tf.set_random_seed(1)                             # to keep consistent results\n",
        "    seed = 3                                          # to keep consistent results\n",
        "    (n_x, m) = X_train.shape                          # (n_x: input size, m : number of examples in the train set)\n",
        "    n_y = Y_train.shape[0]                            # n_y : output size\n",
        "    costs = []                                        # To keep track of the cost\n",
        "    \n",
        "    # Create Placeholders of shape (n_x, n_y)\n",
        "    ### START CODE HERE ### (1 line)\n",
        "    X, Y = create_placeholders(n_x, n_y)\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    # Initialize parameters\n",
        "    ### START CODE HERE ### (1 line)\n",
        "    parameters = initialize_parameters()\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # Forward propagation: Build the forward propagation in the tensorflow graph\n",
        "    ### START CODE HERE ### (1 line)\n",
        "    Z3 = forward_propagation(X, parameters)\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # Cost function: Add cost function to tensorflow graph\n",
        "    ### START CODE HERE ### (1 line)\n",
        "    cost = compute_cost(Z3, Y)\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer.\n",
        "    ### START CODE HERE ### (1 line)\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # Initialize all the variables\n",
        "    init = tf.global_variables_initializer()\n",
        "\n",
        "    # Start the session to compute the tensorflow graph\n",
        "    with tf.Session() as sess:\n",
        "        \n",
        "        # Run the initialization\n",
        "        sess.run(init)\n",
        "        \n",
        "        # Do the training loop\n",
        "        for epoch in range(num_epochs):\n",
        "\n",
        "            epoch_cost = 0.                       # Defines a cost related to an epoch\n",
        "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
        "            seed = seed + 1\n",
        "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
        "\n",
        "            for minibatch in minibatches:\n",
        "\n",
        "                # Select a minibatch\n",
        "                (minibatch_X, minibatch_Y) = minibatch\n",
        "                \n",
        "                # IMPORTANT: The line that runs the graph on a minibatch.\n",
        "                # Run the session to execute the \"optimizer\" and the \"cost\", the feedict should contain a minibatch for (X,Y).\n",
        "                ### START CODE HERE ### (1 line)\n",
        "                _ , minibatch_cost = sess.run([optimizer,cost],feed_dict={X:minibatch_X, Y:minibatch_Y})\n",
        "                ### END CODE HERE ###\n",
        "                \n",
        "                epoch_cost += minibatch_cost / num_minibatches\n",
        "\n",
        "            # Print the cost every epoch\n",
        "            if print_cost == True and epoch % 10 == 0:\n",
        "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
        "            if print_cost == True and epoch % 5 == 0:\n",
        "                costs.append(epoch_cost)\n",
        "                \n",
        "        # plot the cost\n",
        "        plt.plot(np.squeeze(costs))\n",
        "        plt.ylabel('cost')\n",
        "        plt.xlabel('iterations (per tens)')\n",
        "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
        "        plt.show()\n",
        "\n",
        "        # lets save the parameters in a variable\n",
        "        parameters = sess.run(parameters)\n",
        "        print (\"Parameters have been trained!\")\n",
        "\n",
        "        # Calculate the correct predictions\n",
        "        correct_prediction = tf.equal(tf.argmax(Z3), tf.argmax(Y))\n",
        "\n",
        "        # Calculate accuracy on the test set\n",
        "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "\n",
        "        print (\"Train Accuracy:\", accuracy.eval({X: X_train, Y: Y_train}))\n",
        "        #print (\"Test Accuracy:\", accuracy.eval({X: X_test, Y: Y_test}))\n",
        "        \n",
        "        return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CuP4KQUfmyCJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        },
        "outputId": "2ec00104-f1c8-45dc-d812-cd31d3d880af"
      },
      "cell_type": "code",
      "source": [
        "parameters = model(data, y)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "Cost after epoch 0: 2.373775\n",
            "Cost after epoch 10: 0.112290\n",
            "Cost after epoch 20: 0.071873\n",
            "Cost after epoch 30: 0.055281\n",
            "Cost after epoch 40: 0.045461\n",
            "Cost after epoch 50: 0.039404\n",
            "Cost after epoch 60: 0.034967\n",
            "Cost after epoch 70: 0.031431\n",
            "Cost after epoch 80: 0.029534\n",
            "Cost after epoch 90: 0.027200\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niyAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Parameters have been trained!\n",
            "Train Accuracy: 0.99457103\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-_2ACp9Gm5ln",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}